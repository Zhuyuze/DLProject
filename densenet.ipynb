{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary modules here\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    '''\n",
    "        the above mentioned bottleneck, including two conv layer, one's kernel size is 1×1, another's is 3×3\n",
    "\n",
    "        after non-linear operation, concatenate the input to the output\n",
    "    '''\n",
    "    def __init__(self, in_planes, growth_rate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n",
    "        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        \n",
    "        # input and output are concatenated here\n",
    "        out = torch.cat([out,x], 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    '''\n",
    "        transition layer is used for down sampling the feature\n",
    "        \n",
    "        when compress rate is 0.5, out_planes is a half of in_planes\n",
    "    '''\n",
    "    def __init__(self, in_planes, out_planes):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(in_planes)\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.conv(F.relu(self.bn(x)))\n",
    "        # use average pooling change the size of feature map here\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out \n",
    "\n",
    "    \n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=5):\n",
    "        super(DenseNet, self).__init__()\n",
    "        '''\n",
    "        Args:\n",
    "            block: bottleneck\n",
    "            nblock: a list, the elements is number of bottleneck in each denseblock\n",
    "            growth_rate: channel size of bottleneck's output\n",
    "            reduction: \n",
    "        '''\n",
    "        self.growth_rate = growth_rate\n",
    "\n",
    "        num_planes = 2*growth_rate\n",
    "        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n",
    "        \n",
    "        # a DenseBlock and a transition layer\n",
    "        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n",
    "        num_planes += nblocks[0]*growth_rate\n",
    "        # the channel size is superposed, mutiply by reduction to cut it down here, the reduction is also known as compress rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans1 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "        \n",
    "        # a DenseBlock and a transition layer\n",
    "        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n",
    "        num_planes += nblocks[1]*growth_rate\n",
    "        # the channel size is superposed, mutiply by reduction to cut it down here, the reduction is also known as compress rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans2 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        # a DenseBlock and a transition layer\n",
    "        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n",
    "        num_planes += nblocks[2]*growth_rate\n",
    "        # the channel size is superposed, mutiply by reduction to cut it down here, the reduction is also known as compress rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans3 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        # only one DenseBlock \n",
    "        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n",
    "        num_planes += nblocks[3]*growth_rate\n",
    "\n",
    "        # the last part is a linear layer as a classifier\n",
    "        self.bn = nn.BatchNorm2d(num_planes)\n",
    "        self.linear = nn.Linear(2727, num_classes)\n",
    "\n",
    "    def _make_dense_layers(self, block, in_planes, nblock):\n",
    "        layers = []\n",
    "        \n",
    "        # number of non-linear transformations in one DenseBlock depends on the parameter you set\n",
    "        for i in range(nblock):\n",
    "            layers.append(block(in_planes, self.growth_rate))\n",
    "            in_planes += self.growth_rate\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.trans3(self.dense3(out))\n",
    "        out = self.dense4(out)\n",
    "        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def densenet():\n",
    "    return DenseNet(Bottleneck, [12, 14, 12, 14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mydataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_path, img_path):\n",
    "        self.datainfo = pd.read_csv(csv_path)\n",
    "        self.img = np.asarray(self.datainfo.iloc[:, 0])\n",
    "        self.label = np.asarray(self.datainfo.iloc[:, 1])\n",
    "        self.length = len(self.datainfo)\n",
    "        self.trainpath = img_path\n",
    "        self.transform = transforms.Compose([\n",
    "        transforms.Resize(100),\n",
    "        transforms.RandomCrop(100, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize a tensor image with mean and standard variance\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "        print(self.length)\n",
    "        print(self.datainfo.dtypes)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        imgpath = self.trainpath + self.img[index] + '.png'\n",
    "        image = Image.open(imgpath).convert('RGB')\n",
    "        imagetensor = self.transform(image)\n",
    "        imagelabel = self.label[index]\n",
    "        return (imagetensor, imagelabel)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3662\n",
      "id_code      object\n",
      "diagnosis     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "mydataset = Mydataset('./train.csv', './train_images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def train(epoch, model, lossFunction, optimizer, device, trainloader):\n",
    "    \"\"\"train model using loss_fn and optimizer. When this function is called, model trains for one epoch.\n",
    "    Args:\n",
    "        train_loader: train data\n",
    "        model: prediction model\n",
    "        loss_fn: loss function to judge the distance between target and outputs\n",
    "        optimizer: optimize the loss function\n",
    "        get_grad: True, False\n",
    "    output:\n",
    "        total_loss: loss\n",
    "        average_grad2: average grad for hidden 2 in this epoch\n",
    "        average_grad3: average grad for hidden 3 in this epoch\n",
    "    \"\"\"\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    model.train()     # enter train mode\n",
    "    train_loss = 0    # accumulate every batch loss in a epoch\n",
    "    correct = 0       # count when model' prediction is correct i train set\n",
    "    total = 0         # total number of prediction in train set\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device) # load data to gpu device\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        optimizer.zero_grad()            # clear gradients of all optimized torch.Tensors'\n",
    "        outputs = model(inputs)          # forward propagation return the value of softmax function\n",
    "        loss = lossFunction(outputs, targets) #compute loss\n",
    "        loss.backward()                  # compute gradient of loss over parameters \n",
    "        optimizer.step()                 # update parameters with gradient descent \n",
    "\n",
    "        train_loss += loss.item()        # accumulate every batch loss in a epoch\n",
    "        _, predicted = outputs.max(1)    # make prediction according to the outputs\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item() # count how many predictions is correct\n",
    "        \n",
    "        if (batch_idx+1) % 100 == 0:\n",
    "            # print loss and acc\n",
    "            print( 'Train loss: %.3f | Train Acc: %.3f%% (%d/%d)'\n",
    "                % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    print( 'Train loss: %.3f | Train Acc: %.3f%% (%d/%d)'\n",
    "                % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    torch.save(model, './densenet.pt')\n",
    "    \n",
    "    \n",
    "def test(model, lossFunction, optimizer, device, testloader):\n",
    "    \"\"\"\n",
    "    test model's prediction performance on loader.  \n",
    "    When thid function is called, model is evaluated.\n",
    "    Args:\n",
    "        loader: data for evaluation\n",
    "        model: prediction model\n",
    "        loss_fn: loss function to judge the distance between target and outputs\n",
    "    output:\n",
    "        total_loss\n",
    "        accuracy\n",
    "    \"\"\"\n",
    "    global best_acc\n",
    "    model.eval() #enter test mode\n",
    "    test_loss = 0 # accumulate every batch loss in a epoch\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = lossFunction(outputs, targets) #compute loss\n",
    "\n",
    "            test_loss += loss.item() # accumulate every batch loss in a epoch\n",
    "            _, predicted = outputs.max(1) # make prediction according to the outputs\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item() # count how many predictions is correct\n",
    "        # print loss and acc\n",
    "        print('Test Loss: %.3f  | Test Acc: %.3f%% (%d/%d)'\n",
    "            % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "        \n",
    "def data_loader():\n",
    "    # define method of preprocessing data for evaluating\n",
    "    \n",
    "    # prepare dataset by ImageFolder, data should be classified by directory\n",
    "    trainset = mydataset\n",
    "\n",
    "    # Data loader. \n",
    "\n",
    "    # Combines a dataset and a sampler, \n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True)\n",
    "\n",
    "    \n",
    "    return trainloader\n",
    "\n",
    "def run(model, num_epochs):\n",
    "    \n",
    "    # load model into GPU device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    if device == 'cuda':\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    # define the loss function and optimizer\n",
    "\n",
    "    lossFunction = nn.CrossEntropyLoss()\n",
    "    lr = 0.00390625\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    trainloader = data_loader()\n",
    "    for epoch in range(num_epochs):\n",
    "        train(epoch, model, lossFunction, optimizer, device, trainloader)\n",
    "        # test(model, lossFunction, optimizer, device, testloader)\n",
    "        if (epoch + 1) % 30 == 0 :\n",
    "            lr = lr / 10\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Train loss: 1.142 | Train Acc: 58.312% (933/1600)\n",
      "Train loss: 1.047 | Train Acc: 61.344% (1963/3200)\n",
      "Train loss: 1.029 | Train Acc: 62.206% (2278/3662)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:251: UserWarning: Couldn't retrieve source code for container of type DenseNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Bottleneck. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Transition. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n",
      "Train loss: 0.899 | Train Acc: 69.250% (1108/1600)\n",
      "Train loss: 0.894 | Train Acc: 68.719% (2199/3200)\n",
      "Train loss: 0.897 | Train Acc: 68.651% (2514/3662)\n",
      "\n",
      "Epoch: 2\n",
      "Train loss: 0.882 | Train Acc: 67.750% (1084/1600)\n",
      "Train loss: 0.881 | Train Acc: 68.125% (2180/3200)\n",
      "Train loss: 0.888 | Train Acc: 68.159% (2496/3662)\n",
      "\n",
      "Epoch: 3\n",
      "Train loss: 0.837 | Train Acc: 70.375% (1126/1600)\n",
      "Train loss: 0.851 | Train Acc: 69.719% (2231/3200)\n",
      "Train loss: 0.847 | Train Acc: 70.126% (2568/3662)\n",
      "\n",
      "Epoch: 4\n",
      "Train loss: 0.854 | Train Acc: 69.562% (1113/1600)\n",
      "Train loss: 0.828 | Train Acc: 70.812% (2266/3200)\n",
      "Train loss: 0.825 | Train Acc: 70.781% (2592/3662)\n",
      "\n",
      "Epoch: 5\n",
      "Train loss: 0.813 | Train Acc: 70.125% (1122/1600)\n",
      "Train loss: 0.826 | Train Acc: 70.375% (2252/3200)\n",
      "Train loss: 0.822 | Train Acc: 70.781% (2592/3662)\n",
      "\n",
      "Epoch: 6\n",
      "Train loss: 0.839 | Train Acc: 69.750% (1116/1600)\n",
      "Train loss: 0.796 | Train Acc: 71.188% (2278/3200)\n",
      "Train loss: 0.803 | Train Acc: 70.890% (2596/3662)\n",
      "\n",
      "Epoch: 7\n",
      "Train loss: 0.788 | Train Acc: 71.188% (1139/1600)\n",
      "Train loss: 0.795 | Train Acc: 71.500% (2288/3200)\n",
      "Train loss: 0.797 | Train Acc: 71.464% (2617/3662)\n",
      "\n",
      "Epoch: 8\n",
      "Train loss: 0.804 | Train Acc: 71.562% (1145/1600)\n",
      "Train loss: 0.792 | Train Acc: 71.594% (2291/3200)\n",
      "Train loss: 0.790 | Train Acc: 71.764% (2628/3662)\n",
      "\n",
      "Epoch: 9\n",
      "Train loss: 0.779 | Train Acc: 71.562% (1145/1600)\n",
      "Train loss: 0.793 | Train Acc: 71.469% (2287/3200)\n",
      "Train loss: 0.792 | Train Acc: 71.546% (2620/3662)\n",
      "\n",
      "Epoch: 10\n",
      "Train loss: 0.799 | Train Acc: 70.938% (1135/1600)\n",
      "Train loss: 0.776 | Train Acc: 71.969% (2303/3200)\n",
      "Train loss: 0.781 | Train Acc: 71.628% (2623/3662)\n",
      "\n",
      "Epoch: 11\n",
      "Train loss: 0.769 | Train Acc: 72.125% (1154/1600)\n",
      "Train loss: 0.777 | Train Acc: 72.125% (2308/3200)\n",
      "Train loss: 0.784 | Train Acc: 71.819% (2630/3662)\n",
      "\n",
      "Epoch: 12\n",
      "Train loss: 0.784 | Train Acc: 71.688% (1147/1600)\n",
      "Train loss: 0.785 | Train Acc: 71.906% (2301/3200)\n",
      "Train loss: 0.787 | Train Acc: 71.791% (2629/3662)\n",
      "\n",
      "Epoch: 13\n",
      "Train loss: 0.778 | Train Acc: 71.375% (1142/1600)\n",
      "Train loss: 0.771 | Train Acc: 71.750% (2296/3200)\n",
      "Train loss: 0.777 | Train Acc: 71.655% (2624/3662)\n",
      "\n",
      "Epoch: 14\n",
      "Train loss: 0.777 | Train Acc: 72.312% (1157/1600)\n",
      "Train loss: 0.762 | Train Acc: 72.219% (2311/3200)\n",
      "Train loss: 0.772 | Train Acc: 71.873% (2632/3662)\n",
      "\n",
      "Epoch: 15\n",
      "Train loss: 0.772 | Train Acc: 71.750% (1148/1600)\n",
      "Train loss: 0.763 | Train Acc: 72.469% (2319/3200)\n",
      "Train loss: 0.771 | Train Acc: 72.283% (2647/3662)\n",
      "\n",
      "Epoch: 16\n",
      "Train loss: 0.741 | Train Acc: 73.125% (1170/1600)\n",
      "Train loss: 0.762 | Train Acc: 72.062% (2306/3200)\n",
      "Train loss: 0.763 | Train Acc: 72.228% (2645/3662)\n",
      "\n",
      "Epoch: 17\n",
      "Train loss: 0.735 | Train Acc: 72.625% (1162/1600)\n",
      "Train loss: 0.754 | Train Acc: 72.781% (2329/3200)\n",
      "Train loss: 0.761 | Train Acc: 72.638% (2660/3662)\n",
      "\n",
      "Epoch: 18\n",
      "Train loss: 0.760 | Train Acc: 72.000% (1152/1600)\n",
      "Train loss: 0.745 | Train Acc: 73.062% (2338/3200)\n",
      "Train loss: 0.759 | Train Acc: 72.365% (2650/3662)\n",
      "\n",
      "Epoch: 19\n",
      "Train loss: 0.769 | Train Acc: 71.562% (1145/1600)\n",
      "Train loss: 0.766 | Train Acc: 71.812% (2298/3200)\n",
      "Train loss: 0.761 | Train Acc: 72.174% (2643/3662)\n"
     ]
    }
   ],
   "source": [
    "# start training and testing\n",
    "model = densenet()\n",
    "# num_epochs is adjustable\n",
    "run(model, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Train loss: 0.716 | Train Acc: 74.250% (1188/1600)\n",
      "Train loss: 0.725 | Train Acc: 73.875% (2364/3200)\n",
      "Train loss: 0.726 | Train Acc: 73.539% (2693/3662)\n",
      "\n",
      "Epoch: 1\n",
      "Train loss: 0.717 | Train Acc: 73.625% (1178/1600)\n",
      "Train loss: 0.723 | Train Acc: 72.531% (2321/3200)\n",
      "Train loss: 0.721 | Train Acc: 73.020% (2674/3662)\n",
      "\n",
      "Epoch: 2\n",
      "Train loss: 0.734 | Train Acc: 71.812% (1149/1600)\n",
      "Train loss: 0.722 | Train Acc: 72.750% (2328/3200)\n",
      "Train loss: 0.713 | Train Acc: 73.129% (2678/3662)\n",
      "\n",
      "Epoch: 3\n",
      "Train loss: 0.704 | Train Acc: 73.812% (1181/1600)\n",
      "Train loss: 0.717 | Train Acc: 73.188% (2342/3200)\n",
      "Train loss: 0.713 | Train Acc: 73.211% (2681/3662)\n",
      "\n",
      "Epoch: 4\n",
      "Train loss: 0.728 | Train Acc: 73.250% (1172/1600)\n",
      "Train loss: 0.720 | Train Acc: 73.500% (2352/3200)\n",
      "Train loss: 0.715 | Train Acc: 73.758% (2701/3662)\n",
      "\n",
      "Epoch: 5\n",
      "Train loss: 0.717 | Train Acc: 72.625% (1162/1600)\n",
      "Train loss: 0.716 | Train Acc: 73.062% (2338/3200)\n",
      "Train loss: 0.710 | Train Acc: 73.321% (2685/3662)\n",
      "\n",
      "Epoch: 6\n",
      "Train loss: 0.699 | Train Acc: 74.625% (1194/1600)\n",
      "Train loss: 0.722 | Train Acc: 73.250% (2344/3200)\n",
      "Train loss: 0.713 | Train Acc: 73.621% (2696/3662)\n",
      "\n",
      "Epoch: 7\n",
      "Train loss: 0.667 | Train Acc: 75.188% (1203/1600)\n",
      "Train loss: 0.689 | Train Acc: 74.531% (2385/3200)\n",
      "Train loss: 0.698 | Train Acc: 74.140% (2715/3662)\n",
      "\n",
      "Epoch: 8\n",
      "Train loss: 0.686 | Train Acc: 73.875% (1182/1600)\n",
      "Train loss: 0.698 | Train Acc: 73.469% (2351/3200)\n",
      "Train loss: 0.698 | Train Acc: 73.512% (2692/3662)\n",
      "\n",
      "Epoch: 9\n",
      "Train loss: 0.690 | Train Acc: 74.188% (1187/1600)\n",
      "Train loss: 0.705 | Train Acc: 73.656% (2357/3200)\n",
      "Train loss: 0.697 | Train Acc: 73.949% (2708/3662)\n",
      "\n",
      "Epoch: 10\n",
      "Train loss: 0.658 | Train Acc: 76.125% (1218/1600)\n",
      "Train loss: 0.703 | Train Acc: 73.875% (2364/3200)\n",
      "Train loss: 0.705 | Train Acc: 73.867% (2705/3662)\n",
      "\n",
      "Epoch: 11\n",
      "Train loss: 0.685 | Train Acc: 74.062% (1185/1600)\n",
      "Train loss: 0.683 | Train Acc: 74.469% (2383/3200)\n",
      "Train loss: 0.685 | Train Acc: 74.276% (2720/3662)\n",
      "\n",
      "Epoch: 12\n",
      "Train loss: 0.694 | Train Acc: 73.312% (1173/1600)\n",
      "Train loss: 0.695 | Train Acc: 73.969% (2367/3200)\n",
      "Train loss: 0.694 | Train Acc: 74.085% (2713/3662)\n",
      "\n",
      "Epoch: 13\n",
      "Train loss: 0.679 | Train Acc: 73.688% (1179/1600)\n",
      "Train loss: 0.681 | Train Acc: 73.875% (2364/3200)\n",
      "Train loss: 0.689 | Train Acc: 73.839% (2704/3662)\n",
      "\n",
      "Epoch: 14\n",
      "Train loss: 0.684 | Train Acc: 74.125% (1186/1600)\n",
      "Train loss: 0.688 | Train Acc: 73.812% (2362/3200)\n",
      "Train loss: 0.686 | Train Acc: 74.194% (2717/3662)\n",
      "\n",
      "Epoch: 15\n",
      "Train loss: 0.693 | Train Acc: 74.062% (1185/1600)\n",
      "Train loss: 0.680 | Train Acc: 75.281% (2409/3200)\n",
      "Train loss: 0.685 | Train Acc: 74.904% (2743/3662)\n",
      "\n",
      "Epoch: 16\n",
      "Train loss: 0.688 | Train Acc: 73.250% (1172/1600)\n",
      "Train loss: 0.684 | Train Acc: 73.438% (2350/3200)\n",
      "Train loss: 0.683 | Train Acc: 73.566% (2694/3662)\n",
      "\n",
      "Epoch: 17\n",
      "Train loss: 0.692 | Train Acc: 73.688% (1179/1600)\n",
      "Train loss: 0.681 | Train Acc: 74.031% (2369/3200)\n",
      "Train loss: 0.677 | Train Acc: 74.140% (2715/3662)\n",
      "\n",
      "Epoch: 18\n",
      "Train loss: 0.670 | Train Acc: 75.500% (1208/1600)\n",
      "Train loss: 0.677 | Train Acc: 74.750% (2392/3200)\n",
      "Train loss: 0.676 | Train Acc: 74.795% (2739/3662)\n",
      "\n",
      "Epoch: 19\n",
      "Train loss: 0.659 | Train Acc: 74.500% (1192/1600)\n",
      "Train loss: 0.669 | Train Acc: 74.688% (2390/3200)\n",
      "Train loss: 0.673 | Train Acc: 74.686% (2735/3662)\n",
      "\n",
      "Epoch: 20\n",
      "Train loss: 0.659 | Train Acc: 74.812% (1197/1600)\n",
      "Train loss: 0.658 | Train Acc: 75.375% (2412/3200)\n",
      "Train loss: 0.663 | Train Acc: 75.014% (2747/3662)\n",
      "\n",
      "Epoch: 21\n",
      "Train loss: 0.663 | Train Acc: 75.500% (1208/1600)\n",
      "Train loss: 0.667 | Train Acc: 74.844% (2395/3200)\n",
      "Train loss: 0.670 | Train Acc: 74.604% (2732/3662)\n",
      "\n",
      "Epoch: 22\n",
      "Train loss: 0.635 | Train Acc: 75.250% (1204/1600)\n",
      "Train loss: 0.645 | Train Acc: 75.500% (2416/3200)\n",
      "Train loss: 0.653 | Train Acc: 75.150% (2752/3662)\n",
      "\n",
      "Epoch: 23\n",
      "Train loss: 0.644 | Train Acc: 75.562% (1209/1600)\n",
      "Train loss: 0.665 | Train Acc: 74.875% (2396/3200)\n",
      "Train loss: 0.663 | Train Acc: 75.041% (2748/3662)\n",
      "\n",
      "Epoch: 24\n",
      "Train loss: 0.651 | Train Acc: 75.375% (1206/1600)\n",
      "Train loss: 0.665 | Train Acc: 74.594% (2387/3200)\n",
      "Train loss: 0.661 | Train Acc: 74.850% (2741/3662)\n",
      "\n",
      "Epoch: 25\n",
      "Train loss: 0.632 | Train Acc: 75.812% (1213/1600)\n",
      "Train loss: 0.647 | Train Acc: 75.469% (2415/3200)\n",
      "Train loss: 0.655 | Train Acc: 75.287% (2757/3662)\n",
      "\n",
      "Epoch: 26\n",
      "Train loss: 0.632 | Train Acc: 75.875% (1214/1600)\n",
      "Train loss: 0.643 | Train Acc: 74.750% (2392/3200)\n",
      "Train loss: 0.650 | Train Acc: 74.741% (2737/3662)\n",
      "\n",
      "Epoch: 27\n",
      "Train loss: 0.646 | Train Acc: 75.500% (1208/1600)\n",
      "Train loss: 0.653 | Train Acc: 74.750% (2392/3200)\n",
      "Train loss: 0.657 | Train Acc: 74.604% (2732/3662)\n",
      "\n",
      "Epoch: 28\n",
      "Train loss: 0.620 | Train Acc: 76.625% (1226/1600)\n",
      "Train loss: 0.648 | Train Acc: 75.938% (2430/3200)\n",
      "Train loss: 0.649 | Train Acc: 75.532% (2766/3662)\n",
      "\n",
      "Epoch: 29\n",
      "Train loss: 0.624 | Train Acc: 76.500% (1224/1600)\n",
      "Train loss: 0.638 | Train Acc: 76.094% (2435/3200)\n",
      "Train loss: 0.644 | Train Acc: 75.669% (2771/3662)\n",
      "\n",
      "Epoch: 30\n",
      "Train loss: 0.641 | Train Acc: 75.750% (1212/1600)\n",
      "Train loss: 0.617 | Train Acc: 76.469% (2447/3200)\n",
      "Train loss: 0.617 | Train Acc: 76.816% (2813/3662)\n",
      "\n",
      "Epoch: 31\n",
      "Train loss: 0.564 | Train Acc: 79.188% (1267/1600)\n",
      "Train loss: 0.590 | Train Acc: 77.625% (2484/3200)\n",
      "Train loss: 0.595 | Train Acc: 77.526% (2839/3662)\n",
      "\n",
      "Epoch: 32\n",
      "Train loss: 0.580 | Train Acc: 79.125% (1266/1600)\n",
      "Train loss: 0.586 | Train Acc: 77.750% (2488/3200)\n",
      "Train loss: 0.585 | Train Acc: 77.608% (2842/3662)\n",
      "\n",
      "Epoch: 33\n",
      "Train loss: 0.585 | Train Acc: 77.688% (1243/1600)\n",
      "Train loss: 0.579 | Train Acc: 78.031% (2497/3200)\n",
      "Train loss: 0.576 | Train Acc: 78.045% (2858/3662)\n",
      "\n",
      "Epoch: 34\n",
      "Train loss: 0.574 | Train Acc: 77.375% (1238/1600)\n",
      "Train loss: 0.570 | Train Acc: 78.250% (2504/3200)\n",
      "Train loss: 0.577 | Train Acc: 78.017% (2857/3662)\n",
      "\n",
      "Epoch: 35\n",
      "Train loss: 0.568 | Train Acc: 78.125% (1250/1600)\n",
      "Train loss: 0.565 | Train Acc: 78.500% (2512/3200)\n",
      "Train loss: 0.569 | Train Acc: 78.427% (2872/3662)\n",
      "\n",
      "Epoch: 36\n",
      "Train loss: 0.539 | Train Acc: 79.812% (1277/1600)\n",
      "Train loss: 0.558 | Train Acc: 79.000% (2528/3200)\n",
      "Train loss: 0.567 | Train Acc: 78.564% (2877/3662)\n",
      "\n",
      "Epoch: 37\n",
      "Train loss: 0.587 | Train Acc: 78.062% (1249/1600)\n",
      "Train loss: 0.576 | Train Acc: 78.031% (2497/3200)\n",
      "Train loss: 0.574 | Train Acc: 77.826% (2850/3662)\n",
      "\n",
      "Epoch: 38\n",
      "Train loss: 0.539 | Train Acc: 79.688% (1275/1600)\n",
      "Train loss: 0.555 | Train Acc: 79.000% (2528/3200)\n",
      "Train loss: 0.563 | Train Acc: 78.564% (2877/3662)\n",
      "\n",
      "Epoch: 39\n",
      "Train loss: 0.540 | Train Acc: 79.125% (1266/1600)\n",
      "Train loss: 0.559 | Train Acc: 78.531% (2513/3200)\n",
      "Train loss: 0.561 | Train Acc: 78.454% (2873/3662)\n",
      "\n",
      "Epoch: 40\n",
      "Train loss: 0.559 | Train Acc: 79.188% (1267/1600)\n",
      "Train loss: 0.556 | Train Acc: 79.062% (2530/3200)\n",
      "Train loss: 0.556 | Train Acc: 79.001% (2893/3662)\n",
      "\n",
      "Epoch: 41\n",
      "Train loss: 0.573 | Train Acc: 78.625% (1258/1600)\n",
      "Train loss: 0.569 | Train Acc: 78.531% (2513/3200)\n",
      "Train loss: 0.569 | Train Acc: 78.564% (2877/3662)\n",
      "\n",
      "Epoch: 42\n",
      "Train loss: 0.556 | Train Acc: 78.375% (1254/1600)\n",
      "Train loss: 0.553 | Train Acc: 78.719% (2519/3200)\n",
      "Train loss: 0.551 | Train Acc: 78.646% (2880/3662)\n",
      "\n",
      "Epoch: 43\n",
      "Train loss: 0.544 | Train Acc: 78.312% (1253/1600)\n",
      "Train loss: 0.548 | Train Acc: 78.375% (2508/3200)\n",
      "Train loss: 0.552 | Train Acc: 78.673% (2881/3662)\n",
      "\n",
      "Epoch: 44\n",
      "Train loss: 0.522 | Train Acc: 80.812% (1293/1600)\n",
      "Train loss: 0.553 | Train Acc: 78.781% (2521/3200)\n",
      "Train loss: 0.551 | Train Acc: 78.891% (2889/3662)\n",
      "\n",
      "Epoch: 45\n",
      "Train loss: 0.539 | Train Acc: 79.500% (1272/1600)\n",
      "Train loss: 0.550 | Train Acc: 79.219% (2535/3200)\n",
      "Train loss: 0.552 | Train Acc: 79.110% (2897/3662)\n",
      "\n",
      "Epoch: 46\n",
      "Train loss: 0.540 | Train Acc: 79.125% (1266/1600)\n",
      "Train loss: 0.543 | Train Acc: 79.375% (2540/3200)\n",
      "Train loss: 0.543 | Train Acc: 79.356% (2906/3662)\n",
      "\n",
      "Epoch: 47\n",
      "Train loss: 0.527 | Train Acc: 79.312% (1269/1600)\n",
      "Train loss: 0.536 | Train Acc: 79.344% (2539/3200)\n",
      "Train loss: 0.541 | Train Acc: 79.055% (2895/3662)\n",
      "\n",
      "Epoch: 48\n",
      "Train loss: 0.512 | Train Acc: 80.625% (1290/1600)\n",
      "Train loss: 0.530 | Train Acc: 79.438% (2542/3200)\n",
      "Train loss: 0.542 | Train Acc: 79.164% (2899/3662)\n",
      "\n",
      "Epoch: 49\n",
      "Train loss: 0.533 | Train Acc: 79.438% (1271/1600)\n",
      "Train loss: 0.533 | Train Acc: 79.438% (2542/3200)\n",
      "Train loss: 0.531 | Train Acc: 79.547% (2913/3662)\n",
      "\n",
      "Epoch: 50\n",
      "Train loss: 0.538 | Train Acc: 80.125% (1282/1600)\n",
      "Train loss: 0.534 | Train Acc: 79.656% (2549/3200)\n",
      "Train loss: 0.542 | Train Acc: 79.164% (2899/3662)\n",
      "\n",
      "Epoch: 51\n",
      "Train loss: 0.523 | Train Acc: 79.875% (1278/1600)\n",
      "Train loss: 0.533 | Train Acc: 79.656% (2549/3200)\n",
      "Train loss: 0.535 | Train Acc: 79.738% (2920/3662)\n",
      "\n",
      "Epoch: 52\n",
      "Train loss: 0.541 | Train Acc: 79.562% (1273/1600)\n",
      "Train loss: 0.532 | Train Acc: 79.938% (2558/3200)\n",
      "Train loss: 0.529 | Train Acc: 80.093% (2933/3662)\n",
      "\n",
      "Epoch: 53\n",
      "Train loss: 0.516 | Train Acc: 79.375% (1270/1600)\n",
      "Train loss: 0.517 | Train Acc: 79.469% (2543/3200)\n",
      "Train loss: 0.532 | Train Acc: 78.673% (2881/3662)\n",
      "\n",
      "Epoch: 54\n",
      "Train loss: 0.515 | Train Acc: 80.750% (1292/1600)\n",
      "Train loss: 0.526 | Train Acc: 80.250% (2568/3200)\n",
      "Train loss: 0.523 | Train Acc: 80.339% (2942/3662)\n",
      "\n",
      "Epoch: 55\n",
      "Train loss: 0.515 | Train Acc: 79.875% (1278/1600)\n",
      "Train loss: 0.525 | Train Acc: 79.906% (2557/3200)\n",
      "Train loss: 0.527 | Train Acc: 79.902% (2926/3662)\n",
      "\n",
      "Epoch: 56\n",
      "Train loss: 0.501 | Train Acc: 81.250% (1300/1600)\n",
      "Train loss: 0.516 | Train Acc: 80.375% (2572/3200)\n",
      "Train loss: 0.515 | Train Acc: 80.475% (2947/3662)\n",
      "\n",
      "Epoch: 57\n",
      "Train loss: 0.493 | Train Acc: 80.750% (1292/1600)\n",
      "Train loss: 0.512 | Train Acc: 79.875% (2556/3200)\n",
      "Train loss: 0.521 | Train Acc: 79.656% (2917/3662)\n",
      "\n",
      "Epoch: 58\n",
      "Train loss: 0.507 | Train Acc: 79.875% (1278/1600)\n",
      "Train loss: 0.511 | Train Acc: 80.125% (2564/3200)\n",
      "Train loss: 0.516 | Train Acc: 79.765% (2921/3662)\n",
      "\n",
      "Epoch: 59\n",
      "Train loss: 0.501 | Train Acc: 80.062% (1281/1600)\n",
      "Train loss: 0.518 | Train Acc: 80.156% (2565/3200)\n",
      "Train loss: 0.514 | Train Acc: 80.475% (2947/3662)\n"
     ]
    }
   ],
   "source": [
    "run(model, num_epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1928\n",
      "id_code      object\n",
      "diagnosis     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "mytestset = Mydataset('./sample_submission.csv', './test_images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(mytestset, batch_size=1, shuffle=False)\n",
    "result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, testloader):\n",
    "    model.eval() #enter test mode\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1) # make prediction according to the outputs\n",
    "            result.append(predicted.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, 'cuda', testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1928\n",
      "1928\n"
     ]
    }
   ],
   "source": [
    "print(len(result))\n",
    "submission = pd.read_csv('./sample_submission.csv')\n",
    "im = submission.iloc[:, 0]\n",
    "print(len(im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 4 0]\n"
     ]
    }
   ],
   "source": [
    "result = np.array(result).flatten()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame({'id_code':im,'diagnosis':result})\n",
    "df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_code      object\n",
      "diagnosis     int64\n",
      "dtype: object\n",
      "id_code      object\n",
      "diagnosis     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(submission.dtypes)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./densenet.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"./densenet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
