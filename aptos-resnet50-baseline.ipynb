{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Update: changing the batch_size from 32 to 128 when warm up the model, adding Kappa loss from this kernel: https://www.kaggle.com/christofhenkel/weighted-kappa-loss-for-keras-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aptos2019-blindness-detection', 'resnet50']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image, ImageOps\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score, fbeta_score\n",
    "from keras.utils import Sequence\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "WORKERS = 2\n",
    "CHANNEL = 3\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SIZE = 300\n",
    "NUM_CLASSES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f816da39cf8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../input/aptos2019-blindness-detection/train.csv')\n",
    "df_test = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')\n",
    "\n",
    "x = df_train['id_code']\n",
    "y = df_train['diagnosis']\n",
    "\n",
    "x, y = shuffle(x, y, random_state=8)\n",
    "y.hist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3112,)\n",
      "(3112, 5)\n",
      "(550,)\n",
      "(550, 5)\n"
     ]
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=NUM_CLASSES)\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size=0.15,\n",
    "                                                      stratify=y, random_state=8)\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(valid_x.shape)\n",
    "print(valid_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/aleju/imgaug\n",
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "seq = iaa.Sequential([\n",
    "    sometimes(\n",
    "        iaa.OneOf([\n",
    "            iaa.Add((-10, 10), per_channel=0.5),\n",
    "            iaa.Multiply((0.9, 1.1), per_channel=0.5),\n",
    "            iaa.ContrastNormalization((0.9, 1.1), per_channel=0.5)\n",
    "        ])\n",
    "    ),\n",
    "    iaa.Fliplr(0.5),\n",
    "    iaa.Crop(percent=(0, 0.1)),\n",
    "    # iaa.Flipud(0.5)\n",
    "],random_order=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Generator(Sequence):\n",
    "\n",
    "    def __init__(self, image_filenames, labels, batch_size, is_train=True, mix=False):\n",
    "        self.image_filenames, self.labels = image_filenames, labels\n",
    "        self.batch_size = batch_size\n",
    "        self.is_train = is_train\n",
    "        self.on_epoch_end()\n",
    "        self.is_mix = mix\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_filenames) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.image_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        if(self.is_train):\n",
    "            return self.train_generate(batch_x, batch_y)\n",
    "        return self.valid_generate(batch_x, batch_y)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if(self.is_train):\n",
    "            self.image_filenames, self.labels = shuffle(self.image_filenames, self.labels)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    def mix_up(self, x, y):\n",
    "        lam = np.random.beta(0.2, 0.4)\n",
    "        ori_index = np.arange(int(len(x)))\n",
    "        index_array = np.arange(int(len(x)))\n",
    "        np.random.shuffle(index_array)        \n",
    "        \n",
    "        mixed_x = lam * x[ori_index] + (1 - lam) * x[index_array]\n",
    "        mixed_y = lam * y[ori_index] + (1 - lam) * y[index_array]\n",
    "        \n",
    "        return mixed_x, mixed_y\n",
    "\n",
    "    def train_generate(self, batch_x, batch_y):\n",
    "        batch_images = []\n",
    "        for (sample, label) in zip(batch_x, batch_y):\n",
    "            img = cv2.imread('../input/aptos2019-blindness-detection/train_images/'+sample+'.png')\n",
    "            img = cv2.resize(img, (SIZE, SIZE))\n",
    "            img = seq.augment_image(img)\n",
    "            batch_images.append(img)\n",
    "        batch_images = np.array(batch_images, np.float32) / 255\n",
    "        batch_y = np.array(batch_y, np.float32)\n",
    "        if(self.is_mix):\n",
    "            batch_images, batch_y = self.mix_up(batch_images, batch_y)\n",
    "        return batch_images, batch_y\n",
    "\n",
    "    def valid_generate(self, batch_x, batch_y):\n",
    "        batch_images = []\n",
    "        for (sample, label) in zip(batch_x, batch_y):\n",
    "            img = cv2.imread('../input/aptos2019-blindness-detection/train_images/'+sample+'.png')\n",
    "            img = cv2.resize(img, (SIZE, SIZE))\n",
    "            batch_images.append(img)\n",
    "        batch_images = np.array(batch_images, np.float32) / 255\n",
    "        batch_y = np.array(batch_y, np.float32)\n",
    "        return batch_images, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import (Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D,\n",
    "                          BatchNormalization, Input, Conv2D, GlobalAveragePooling2D)\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_out):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    base_model = ResNet50(include_top=False,\n",
    "                   weights=None,\n",
    "                   input_tensor=input_tensor)\n",
    "    base_model.load_weights('../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    final_output = Dense(n_out, activation='softmax', name='final_output')(x)\n",
    "    model = Model(input_tensor, final_output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create callbacks list\n",
    "from keras.callbacks import (ModelCheckpoint, LearningRateScheduler,\n",
    "                             EarlyStopping, ReduceLROnPlateau,CSVLogger)\n",
    "\n",
    "epochs = 30; batch_size = 32\n",
    "checkpoint = ModelCheckpoint('../working/Resnet50.h5', monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, \n",
    "                                   verbose=1, mode='auto', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=9)\n",
    "\n",
    "csv_logger = CSVLogger(filename='../working/training_log.csv',\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "# callbacks_list = [checkpoint, csv_logger, reduceLROnPlat, early]\n",
    "\n",
    "train_generator = My_Generator(train_x, train_y, 128, is_train=True)\n",
    "train_mixup = My_Generator(train_x, train_y, batch_size, is_train=True, mix=True)\n",
    "valid_generator = My_Generator(valid_x, valid_y, batch_size, is_train=False)\n",
    "\n",
    "model = create_model(\n",
    "    input_shape=(SIZE,SIZE,3), \n",
    "    n_out=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference link: https://www.kaggle.com/christofhenkel/weighted-kappa-loss-for-keras-tensorflow\n",
    "def kappa_loss(y_true, y_pred, y_pow=2, eps=1e-12, N=5, bsize=32, name='kappa'):\n",
    "    \"\"\"A continuous differentiable approximation of discrete kappa loss.\n",
    "        Args:\n",
    "            y_pred: 2D tensor or array, [batch_size, num_classes]\n",
    "            y_true: 2D tensor or array,[batch_size, num_classes]\n",
    "            y_pow: int,  e.g. y_pow=2\n",
    "            N: typically num_classes of the model\n",
    "            bsize: batch_size of the training or validation ops\n",
    "            eps: a float, prevents divide by zero\n",
    "            name: Optional scope/name for op_scope.\n",
    "        Returns:\n",
    "            A tensor with the kappa loss.\"\"\"\n",
    "\n",
    "    with tf.name_scope(name):\n",
    "        y_true = tf.to_float(y_true)\n",
    "        repeat_op = tf.to_float(tf.tile(tf.reshape(tf.range(0, N), [N, 1]), [1, N]))\n",
    "        repeat_op_sq = tf.square((repeat_op - tf.transpose(repeat_op)))\n",
    "        weights = repeat_op_sq / tf.to_float((N - 1) ** 2)\n",
    "    \n",
    "        pred_ = y_pred ** y_pow\n",
    "        try:\n",
    "            pred_norm = pred_ / (eps + tf.reshape(tf.reduce_sum(pred_, 1), [-1, 1]))\n",
    "        except Exception:\n",
    "            pred_norm = pred_ / (eps + tf.reshape(tf.reduce_sum(pred_, 1), [bsize, 1]))\n",
    "    \n",
    "        hist_rater_a = tf.reduce_sum(pred_norm, 0)\n",
    "        hist_rater_b = tf.reduce_sum(y_true, 0)\n",
    "    \n",
    "        conf_mat = tf.matmul(tf.transpose(pred_norm), y_true)\n",
    "    \n",
    "        nom = tf.reduce_sum(weights * conf_mat)\n",
    "        denom = tf.reduce_sum(weights * tf.matmul(\n",
    "            tf.reshape(hist_rater_a, [N, 1]), tf.reshape(hist_rater_b, [1, N])) /\n",
    "                              tf.to_float(bsize))\n",
    "    \n",
    "        return nom*0.5 / (denom + eps) + categorical_crossentropy(y_true, y_pred)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "25/25 [==============================] - 181s 7s/step - loss: 1.7247\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 165s 7s/step - loss: 0.7525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7d3dc835c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# warm up model\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "for i in range(-3,0):\n",
    "    model.layers[i].trainable = True\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(1e-3))\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(float(len(train_y)) / float(128)),\n",
    "    epochs=2,\n",
    "    workers=WORKERS, use_multiprocessing=True,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "98/98 [==============================] - 312s 3s/step - loss: 0.8215 - acc: 0.7389 - val_loss: 0.5472 - val_acc: 0.8127\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.54723, saving model to ../working/Resnet50.h5\n",
      "Epoch 2/30\n",
      "98/98 [==============================] - 220s 2s/step - loss: 0.7255 - acc: 0.7746 - val_loss: 0.5329 - val_acc: 0.8109\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.54723 to 0.53291, saving model to ../working/Resnet50.h5\n",
      "Epoch 3/30\n",
      "98/98 [==============================] - 240s 2s/step - loss: 0.7228 - acc: 0.7825 - val_loss: 0.5664 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.53291\n",
      "Epoch 4/30\n",
      "98/98 [==============================] - 240s 2s/step - loss: 0.6606 - acc: 0.7940 - val_loss: 0.6473 - val_acc: 0.8036\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.53291\n",
      "Epoch 5/30\n",
      "98/98 [==============================] - 239s 2s/step - loss: 0.6606 - acc: 0.8093 - val_loss: 0.5115 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.53291 to 0.51148, saving model to ../working/Resnet50.h5\n",
      "Epoch 6/30\n",
      "98/98 [==============================] - 240s 2s/step - loss: 0.6017 - acc: 0.8268 - val_loss: 0.5190 - val_acc: 0.8273\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51148\n",
      "Epoch 7/30\n",
      "98/98 [==============================] - 237s 2s/step - loss: 0.6194 - acc: 0.8272 - val_loss: 0.6147 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51148\n",
      "Epoch 8/30\n",
      "98/98 [==============================] - 239s 2s/step - loss: 0.6007 - acc: 0.8278 - val_loss: 0.5075 - val_acc: 0.8273\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.51148 to 0.50753, saving model to ../working/Resnet50.h5\n",
      "Epoch 9/30\n",
      "98/98 [==============================] - 237s 2s/step - loss: 0.5482 - acc: 0.8463 - val_loss: 0.4923 - val_acc: 0.8273\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.50753 to 0.49234, saving model to ../working/Resnet50.h5\n",
      "Epoch 10/30\n",
      "98/98 [==============================] - 237s 2s/step - loss: 0.5686 - acc: 0.8454 - val_loss: 0.5286 - val_acc: 0.8091\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.49234\n",
      "Epoch 11/30\n",
      "98/98 [==============================] - 253s 3s/step - loss: 0.5788 - acc: 0.8463 - val_loss: 0.4918 - val_acc: 0.8291\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.49234 to 0.49184, saving model to ../working/Resnet50.h5\n",
      "Epoch 12/30\n",
      "98/98 [==============================] - 280s 3s/step - loss: 0.5056 - acc: 0.8741 - val_loss: 0.5835 - val_acc: 0.8309\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.49184\n",
      "Epoch 13/30\n",
      "98/98 [==============================] - 282s 3s/step - loss: 0.5422 - acc: 0.8626 - val_loss: 0.6080 - val_acc: 0.8109\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.49184\n",
      "Epoch 14/30\n",
      "98/98 [==============================] - 284s 3s/step - loss: 0.5064 - acc: 0.8823 - val_loss: 0.6717 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.49184\n",
      "Epoch 15/30\n",
      "98/98 [==============================] - 283s 3s/step - loss: 0.5072 - acc: 0.8801 - val_loss: 0.6029 - val_acc: 0.8018\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.49184\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 16/30\n",
      "98/98 [==============================] - 282s 3s/step - loss: 0.4370 - acc: 0.9008 - val_loss: 0.5898 - val_acc: 0.7836\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.49184\n",
      "Epoch 17/30\n",
      "98/98 [==============================] - 285s 3s/step - loss: 0.3833 - acc: 0.9193 - val_loss: 0.6269 - val_acc: 0.7964\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.49184\n",
      "Epoch 18/30\n",
      "98/98 [==============================] - 285s 3s/step - loss: 0.3478 - acc: 0.9423 - val_loss: 0.5356 - val_acc: 0.8164\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.49184\n",
      "Epoch 19/30\n",
      "98/98 [==============================] - 283s 3s/step - loss: 0.4023 - acc: 0.9289 - val_loss: 0.6297 - val_acc: 0.7800\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.49184\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 20/30\n",
      "98/98 [==============================] - 281s 3s/step - loss: 0.3307 - acc: 0.9423 - val_loss: 0.5672 - val_acc: 0.8073\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.49184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7d3dc83b70>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train all layers\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "callbacks_list = [checkpoint, csv_logger, reduceLROnPlat, early]\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            # loss=kappa_loss,\n",
    "            optimizer=Adam(lr=1e-4),\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(\n",
    "    train_mixup,\n",
    "    steps_per_epoch=np.ceil(float(len(train_x)) / float(batch_size)),\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=np.ceil(float(len(valid_x)) / float(batch_size)),\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    workers=1, use_multiprocessing=False,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\n",
    "model.load_weights('../working/Resnet50.h5')\n",
    "predicted = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1928it [01:22, 23.45it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, name in tqdm(enumerate(submit['id_code'])):\n",
    "    path = os.path.join('../input/aptos2019-blindness-detection/test_images/', name+'.png')\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.resize(image, (SIZE, SIZE))\n",
    "    score_predict = model.predict((image[np.newaxis])/255)\n",
    "    label_predict = np.argmax(score_predict)\n",
    "    predicted.append(str(label_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0005cfc8afb6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003f0afdcd15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006efc72b638</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00836aaacf06</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>009245722fa4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_code diagnosis\n",
       "0  0005cfc8afb6         1\n",
       "1  003f0afdcd15         2\n",
       "2  006efc72b638         3\n",
       "3  00836aaacf06         2\n",
       "4  009245722fa4         3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit['diagnosis'] = predicted\n",
    "submit.to_csv('submission.csv', index=False)\n",
    "submit.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
